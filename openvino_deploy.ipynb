{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52fb25f6",
   "metadata": {},
   "source": [
    "### Pytorch --> ONNX\n",
    "\n",
    "As it was mentioned before, to deploy our model with OpenVINO, we may need to convert our model to onnx first. So the following steps are the same with \"onnx_deploy.ipynb\" tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d203e915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from networkFT import Net\n",
    "\n",
    "\"\"\" Loading the trained model\"\"\"\n",
    "model = Net(3) \n",
    "model.load_state_dict(torch.load('./outputs_FT/model.pt'))\n",
    "\n",
    "\"\"\" create an example input having same shape with the expected input for the trained model\"\"\"\n",
    "\n",
    "x = torch.randn((1, 3, 240, 240))\n",
    "\n",
    "\"\"\" convert it to onnx\"\"\"\n",
    "torch.onnx.export(model,       # model to convert\n",
    "                  x,         # model input\n",
    "                  \"onnx/model.onnx\",  # output model name\n",
    "                  export_params=True, # store the trained weights\n",
    "                  opset_version=11,   # the ONNX version\n",
    "                  do_constant_folding=True, # remove randomness, make inference faster\n",
    "                  input_names= ['input'], # set model input names    \n",
    "                  output_names=['output'], # set model output names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee0ff31",
   "metadata": {},
   "source": [
    "### ONNX --> OpenVINO\n",
    "\n",
    "Using the following terminal command, we will convert our onnx model to OpenVINO. We can determine the datatype with --data_type parameter. Note that we trained our model with float32 type, which means every single weight we have in our model is float32. For now, we will use the same data type but note that quantization, which means decreasing the data type of your model, may give you a smaller model size with a not significant accuracy loss. Ä±magine you dont store your parameters in float32 range -4 byte- but in uint8 range -1 byte-, your parameter amount would be the same with a smaller model size. Since you round your weights and cut the decimal numbers, you may have little accuracy loss though.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4cc5de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ WARNING ]  Use of deprecated cli option --data_type detected. Option use in the following releases will be fatal. \n",
      "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
      "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /home/yca/educative/docker_image_jp/openvino/model.xml\n",
      "[ SUCCESS ] BIN file: /home/yca/educative/docker_image_jp/openvino/model.bin\n"
     ]
    }
   ],
   "source": [
    "!mo --input_model \"onnx/model.onnx\" --input_shape \"[1, 3, 240, 240]\" --data_type FP32 --output_dir \"openvino/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9fa4b0",
   "metadata": {},
   "source": [
    "### Inference with OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa5925d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" prepare input \"\"\"\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img1 = cv2.imread(\"data_flowers/daisy/100080576_f52e8ee070_n.jpg\") \n",
    "img1 = cv2.resize(img1, (240,240), interpolation = cv2.INTER_AREA) # (240, 240, 3)\n",
    "img1 = np.reshape(img1, (1,3,240,240)) #(1, 3, 240, 240)\n",
    "img1 = img1.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2f8ba08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]]\n",
      "daisy\n"
     ]
    }
   ],
   "source": [
    "from openvino.runtime import Core\n",
    "\n",
    "# Load the network in OpenVINO Runtime.\n",
    "ie = Core()\n",
    "model_ir = ie.read_model(model=\"openvino/model.xml\")\n",
    "compiled_model_ir = ie.compile_model(model=model_ir, device_name=\"CPU\")\n",
    "\n",
    "# Get input and output layers.\n",
    "output_layer_ir = compiled_model_ir.output(0)\n",
    "\n",
    "# Run inference on the input image.\n",
    "scores = compiled_model_ir([img1])[output_layer_ir]\n",
    "\n",
    "\"\"\" check the result\"\"\"\n",
    "class_names = ['dandelion', 'rose', 'daisy'] \n",
    "print(scores)\n",
    "scores = list(scores[0])\n",
    "print(class_names[scores.index(max(scores))]) #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8083dd1b",
   "metadata": {},
   "source": [
    "### Compare Inference Time\n",
    "\n",
    "We already checked the inference time for Pytorch and ONNX with onnxruntime for python. There are two additional options with OpenVINO: \n",
    "\n",
    "1. ONNX model with OpenVINO runtime for python \n",
    "2. OpenVINO model with OpenVINO runtime for python \n",
    "\n",
    "Yes, it is also possible to run ONNX models directly in OpenVINO engine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f78cdc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference took 0.0051517486572265625 seconds for 1 image\n"
     ]
    }
   ],
   "source": [
    "\"\"\" ONNX in OpenVINO runtime\"\"\"\n",
    "import time\n",
    "\n",
    "# Load the network to OpenVINO Runtime.\n",
    "ie = Core()\n",
    "model_onnx = ie.read_model(model=\"onnx/model.onnx\")\n",
    "compiled_model_onnx = ie.compile_model(model=model_onnx, device_name=\"CPU\")\n",
    "\n",
    "output_layer_onnx = compiled_model_onnx.output(0)\n",
    "\n",
    "# Run inference on the input image.\n",
    "\n",
    "start_inference = time.time()\n",
    "\n",
    "res_onnx = compiled_model_onnx([img1])[output_layer_onnx]\n",
    "\n",
    "end_inference = time.time()\n",
    "\n",
    "print(\"inference took\", end_inference-start_inference, \"seconds for 1 image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ffabc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference took 0.0058476924896240234 seconds for 1 image\n"
     ]
    }
   ],
   "source": [
    "\"\"\" OpenVINO in OpenVINO runtime\"\"\"\n",
    "\n",
    "# Run inference on the input image.\n",
    "\n",
    "start_inference = time.time()\n",
    "\n",
    "scores = compiled_model_ir([img1])[output_layer_ir]\n",
    "\n",
    "end_inference = time.time()\n",
    "\n",
    "print(\"inference took\", end_inference-start_inference, \"seconds for 1 image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab5d6bf",
   "metadata": {},
   "source": [
    "It seems that ONNX in onnxruntime overperforms these results while any converted model gives a faster inference than inference with PyTorch! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
