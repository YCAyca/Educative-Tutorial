{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31e571c7",
   "metadata": {},
   "source": [
    "## From PyTorch to ONNX\n",
    "\n",
    "Its a very easy process with only 1 line coding thanks to torch.onnx.export function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57c7c283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from networkFT import Net\n",
    "\n",
    "\"\"\" Loading the trained model\"\"\"\n",
    "model = Net(3) \n",
    "model.load_state_dict(torch.load('./outputs_FT/model.pt'))\n",
    "\n",
    "\"\"\" create an example input having same shape with the expected input for the trained model\"\"\"\n",
    "\n",
    "x = torch.randn((1, 3, 240, 240))\n",
    "\n",
    "\"\"\" convert it to onnx\"\"\"\n",
    "torch.onnx.export(model,       # model to convert\n",
    "                  x,         # model input\n",
    "                  \"onnx/model.onnx\",  # output model name\n",
    "                  export_params=True, # store the trained weights\n",
    "                  opset_version=11,   # the ONNX version\n",
    "                  do_constant_folding=True, # remove randomness, make inference faster\n",
    "                  input_names= ['input'], # set model input names    \n",
    "                  output_names=['output'], # set model output names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b91a977",
   "metadata": {},
   "source": [
    "## Inference with ONNX\n",
    "\n",
    "We used onnx framework to make the conversion. For inference, we will need onnx runtime to create our deployment pipeline working on onnx model. Note that now we are done with torch Tensors and we will send our input images directly in the form we read them. Only two things to obtain the required form for input data:\n",
    "1. OpenCV read the images in H,W,C order and we need to reshape it to C,H,W as well as adding 1 dimension for the batch size\n",
    "2. Converting the image pixel values from uint8 to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1ce1a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1.]\n",
      "daisy\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\"\"\" start onnx runtime session\"\"\"\n",
    "session = ort.InferenceSession(\"onnx/model.onnx\") #start onnx engine for our onnx model\n",
    "\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "\n",
    "\"\"\" prepare input \"\"\"\n",
    "img1 = cv2.imread(\"data_flowers/daisy/100080576_f52e8ee070_n.jpg\") \n",
    "img1 = cv2.resize(img1, (240,240), interpolation = cv2.INTER_AREA) # (240, 240, 3)\n",
    "img1 = np.reshape(img1, (1,3,240,240)) #(1, 3, 240, 240)\n",
    "img1 = img1.astype(np.float32)\n",
    "\n",
    "\n",
    "\"\"\" run \"\"\"\n",
    "scores = session.run([output_name], {input_name: img1})[0]\n",
    "\n",
    "\"\"\" check the result\"\"\"\n",
    "class_names = ['dandelion', 'rose', 'daisy'] # remember our class_idx in training, output node 0: dandelion, output node 1:rose and output node 2: daisy\n",
    "print(scores[0]) # prediction probabilities for each class\n",
    "scores = list(scores[0])\n",
    "print(class_names[scores.index(max(scores))]) # label having the maximum probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a50354",
   "metadata": {},
   "source": [
    "## Compare Inference Time\n",
    "\n",
    "We managed to convert our model and make inference in Python using onnxruntime. But what is changed exactly?\n",
    "How fast is our model now? Let's compare our inference speed for different batch size\n",
    "\n",
    "### Inference time with ONNX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "590790a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference took 0.004109859466552734 seconds for 1 image\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_inference = time.time()\n",
    "\n",
    "scores = session.run([output_name], {input_name: img1})[0]\n",
    "\n",
    "end_inference = time.time()\n",
    "\n",
    "print(\"inference took\", end_inference-start_inference, \"seconds for 1 image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88865ae",
   "metadata": {},
   "source": [
    "### Inference time with PyTorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b888bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference took 0.0071868896484375 seconds for 1 image\n"
     ]
    }
   ],
   "source": [
    "from torchvision import  transforms\n",
    "\n",
    "model = Net(3) \n",
    "model.load_state_dict(torch.load('./outputs_FT/model.pt'))\n",
    "\n",
    "inf_transforms=transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((240,240)),\n",
    "    transforms.ToTensor()\n",
    "])  \n",
    "\n",
    "img1 = cv2.imread(\"data_flowers/daisy/100080576_f52e8ee070_n.jpg\") \n",
    "img1 = inf_transforms(img1)\n",
    "img1 = img1.unsqueeze(0)\n",
    "\n",
    "start_inference = time.time()\n",
    "\n",
    "outputs = model(img1.float())\n",
    "\n",
    "end_inference = time.time()\n",
    "\n",
    "print(\"inference took\", end_inference-start_inference, \"seconds for 1 image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b00db6",
   "metadata": {},
   "source": [
    "Yep! As we see ONNX inference is 2x faster than PyTorch inference. It may seem to be a little change since we talk about miliseconds, but imagine the time savings you would obtain while processing 1000 images one by one and for many real-time projects, even miliseconds are important! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
