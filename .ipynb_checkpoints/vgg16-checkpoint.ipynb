{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0964f4a4",
   "metadata": {},
   "source": [
    "Until now, we created indiviual blocks for convolution, max pooling, activation function and we gave the output of the layer as input to the next layer explicitly. While creating a whole architecture, we can use nn.Sequential() function to apply all the layers given to this function sequentially instead of carrying the output as the input of the next layer each time.\n",
    "\n",
    "Below, we create a class VGG16 from torch's Module class which will give us some utilities to examine our architecture better in the further steps. By initializing this class we obtain the architecture we want, and in forward() function we are able to give an input to make 1 whole forward pass through our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61c73ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "class VGG16(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.convolutions = nn.Sequential(\n",
    "            # conv1\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            \n",
    "            # conv2\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "            # conv3\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "            # conv4\n",
    "            nn.Conv2d(256, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "            # conv5\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.fully_connected = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, nb_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, img):\n",
    "\n",
    "        # Apply convolution operations\n",
    "        x = self.convolutions(img)\n",
    "        # Reshape\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Apply fully connected operations\n",
    "        x = self.fully_connected(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73587bc0",
   "metadata": {},
   "source": [
    "Create an VGG16 model object with 3 classes: daisy, dandelion and rose. Note that VGG16 architecture is created for ImageNet dataset having 1000 classes originally. Print model summary to see the layers in the architecture with their output shapes and parameter amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45f172b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [1, 64, 224, 224]           1,792\n",
      "              ReLU-2          [1, 64, 224, 224]               0\n",
      "            Conv2d-3          [1, 64, 224, 224]          36,928\n",
      "              ReLU-4          [1, 64, 224, 224]               0\n",
      "         MaxPool2d-5          [1, 64, 112, 112]               0\n",
      "            Conv2d-6         [1, 128, 112, 112]          73,856\n",
      "              ReLU-7         [1, 128, 112, 112]               0\n",
      "            Conv2d-8         [1, 128, 112, 112]         147,584\n",
      "              ReLU-9         [1, 128, 112, 112]               0\n",
      "        MaxPool2d-10           [1, 128, 56, 56]               0\n",
      "           Conv2d-11           [1, 256, 56, 56]         295,168\n",
      "             ReLU-12           [1, 256, 56, 56]               0\n",
      "           Conv2d-13           [1, 256, 56, 56]         590,080\n",
      "             ReLU-14           [1, 256, 56, 56]               0\n",
      "           Conv2d-15           [1, 256, 56, 56]         590,080\n",
      "             ReLU-16           [1, 256, 56, 56]               0\n",
      "        MaxPool2d-17           [1, 256, 28, 28]               0\n",
      "           Conv2d-18           [1, 512, 28, 28]       1,180,160\n",
      "             ReLU-19           [1, 512, 28, 28]               0\n",
      "           Conv2d-20           [1, 512, 28, 28]       2,359,808\n",
      "             ReLU-21           [1, 512, 28, 28]               0\n",
      "           Conv2d-22           [1, 512, 28, 28]       2,359,808\n",
      "             ReLU-23           [1, 512, 28, 28]               0\n",
      "        MaxPool2d-24           [1, 512, 14, 14]               0\n",
      "           Conv2d-25           [1, 512, 14, 14]       2,359,808\n",
      "             ReLU-26           [1, 512, 14, 14]               0\n",
      "           Conv2d-27           [1, 512, 14, 14]       2,359,808\n",
      "             ReLU-28           [1, 512, 14, 14]               0\n",
      "           Conv2d-29           [1, 512, 14, 14]       2,359,808\n",
      "             ReLU-30           [1, 512, 14, 14]               0\n",
      "        MaxPool2d-31             [1, 512, 7, 7]               0\n",
      "           Linear-32                  [1, 4096]     102,764,544\n",
      "             ReLU-33                  [1, 4096]               0\n",
      "          Dropout-34                  [1, 4096]               0\n",
      "           Linear-35                  [1, 4096]      16,781,312\n",
      "             ReLU-36                  [1, 4096]               0\n",
      "          Dropout-37                  [1, 4096]               0\n",
      "           Linear-38                     [1, 3]          12,291\n",
      "          Softmax-39                     [1, 3]               0\n",
      "================================================================\n",
      "Total params: 134,272,835\n",
      "Trainable params: 134,272,835\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 218.58\n",
      "Params size (MB): 512.21\n",
      "Estimated Total Size (MB): 731.37\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "nb_classes = 3 # 1000 in \n",
    "model = VGG16(nb_classes)\n",
    "\n",
    "summary(model, input_size = (3, 224, 224), batch_size = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a07def6",
   "metadata": {},
   "source": [
    "Send one image through network and check the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa2339c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3363, 0.3355, 0.3283]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor(0.3363, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread(\"data_flowers/daisy/100080576_f52e8ee070_n.jpg\") \n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor()\n",
    "])  \n",
    "\n",
    "img = transform(img)\n",
    "img = img.unsqueeze(0)\n",
    "\n",
    "output = model(img)\n",
    "\n",
    "print(output)\n",
    "\n",
    "print(torch.max(output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b6d179",
   "metadata": {},
   "source": [
    "There is 3 possibilities for 3 classes. The values seem to be very close to each other, so even though we choose the maximum among them, it doesnt feel that it will work well to distinguish these 3 types of flower right?\n",
    "Its because we didnt train our model yet! We just created he architecture and started to use it for inference. Well, it was just to show you the logic, wait until we are done with the training step ;)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
